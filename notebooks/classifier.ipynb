{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/raw/1000dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360Wr96ywrCQq4kTAJ8Pq6</td>\n",
       "      <td>['Melanie Martinez']</td>\n",
       "      <td>80</td>\n",
       "      <td>220013</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.593</td>\n",
       "      <td>9</td>\n",
       "      <td>-6.024</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.04670</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.3730</td>\n",
       "      <td>157.993</td>\n",
       "      <td>4</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3yqoD9SYDfwvNEdU2NEpFx</td>\n",
       "      <td>['Rovalio']</td>\n",
       "      <td>50</td>\n",
       "      <td>146891</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.913</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.42100</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>150.012</td>\n",
       "      <td>4</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ygvZOXrIeVL4xZmAWJT2C</td>\n",
       "      <td>['Billie Eilish']</td>\n",
       "      <td>72</td>\n",
       "      <td>208155</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.309</td>\n",
       "      <td>8</td>\n",
       "      <td>-10.956</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>0.79500</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>104.745</td>\n",
       "      <td>4</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6HMT7AzGA5wmn5m2t1BuZR</td>\n",
       "      <td>['Alan Walker', 'Steve Aoki', 'ISÁK', 'Omar No...</td>\n",
       "      <td>54</td>\n",
       "      <td>216053</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.819</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.267</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.00911</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>89.952</td>\n",
       "      <td>4</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0uLPKDuIhkIhttiwezNdiG</td>\n",
       "      <td>['Alec Benjamin']</td>\n",
       "      <td>66</td>\n",
       "      <td>171539</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.237</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>0.66400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>94.049</td>\n",
       "      <td>4</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id                                            artists  \\\n",
       "0  360Wr96ywrCQq4kTAJ8Pq6                               ['Melanie Martinez']   \n",
       "1  3yqoD9SYDfwvNEdU2NEpFx                                        ['Rovalio']   \n",
       "2  2ygvZOXrIeVL4xZmAWJT2C                                  ['Billie Eilish']   \n",
       "3  6HMT7AzGA5wmn5m2t1BuZR  ['Alan Walker', 'Steve Aoki', 'ISÁK', 'Omar No...   \n",
       "4  0uLPKDuIhkIhttiwezNdiG                                  ['Alec Benjamin']   \n",
       "\n",
       "   popularity  duration_ms  danceability  energy  key  loudness  mode  \\\n",
       "0          80       220013         0.576   0.593    9    -6.024     0   \n",
       "1          50       146891         0.586   0.402    0   -10.913     0   \n",
       "2          72       208155         0.444   0.309    8   -10.956     1   \n",
       "3          54       216053         0.521   0.819    6    -7.267     0   \n",
       "4          66       171539         0.787   0.558    0    -5.237     1   \n",
       "\n",
       "   speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0       0.0306       0.04670          0.000767     0.237   0.3730  157.993   \n",
       "1       0.0301       0.42100          0.903000     0.135   0.1550  150.012   \n",
       "2       0.0620       0.79500          0.132000     0.352   0.0875  104.745   \n",
       "3       0.0413       0.00911          0.002270     0.086   0.1840   89.952   \n",
       "4       0.0414       0.66400          0.000000     0.347   0.7060   94.049   \n",
       "\n",
       "   time_signature track_genre  \n",
       "0               4     electro  \n",
       "1               4     electro  \n",
       "2               4     electro  \n",
       "3               4     electro  \n",
       "4               4     electro  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only track_id and track_genre\n",
    "dfg = df[['track_id', 'track_genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   track_id     1000 non-null   object\n",
      " 1   track_genre  1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360Wr96ywrCQq4kTAJ8Pq6</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3yqoD9SYDfwvNEdU2NEpFx</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2ygvZOXrIeVL4xZmAWJT2C</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6HMT7AzGA5wmn5m2t1BuZR</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0uLPKDuIhkIhttiwezNdiG</td>\n",
       "      <td>electro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id track_genre\n",
       "0  360Wr96ywrCQq4kTAJ8Pq6     electro\n",
       "1  3yqoD9SYDfwvNEdU2NEpFx     electro\n",
       "2  2ygvZOXrIeVL4xZmAWJT2C     electro\n",
       "3  6HMT7AzGA5wmn5m2t1BuZR     electro\n",
       "4  0uLPKDuIhkIhttiwezNdiG     electro"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg.info()\n",
    "dfg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform one-hot encoding\n",
    "#dfg_e = pd.get_dummies(dfg, columns=['track_genre'])\n",
    "#\n",
    "## Identify the one-hot-encoded columns\n",
    "#one_hot_columns = [col for col in dfg_e.columns if 'track_genre_' in col]\n",
    "#\n",
    "## Convert only the one-hot-encoded columns to integers\n",
    "#dfg_e[one_hot_columns] = dfg_e[one_hot_columns].astype(int)\n",
    "#\n",
    "## Verify the result\n",
    "#dfg_e.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...</td>\n",
       "      <td>hip-hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...</td>\n",
       "      <td>hip-hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...</td>\n",
       "      <td>hip-hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...</td>\n",
       "      <td>hip-hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...</td>\n",
       "      <td>hip-hop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path    class\n",
       "0  ../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...  hip-hop\n",
       "1  ../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...  hip-hop\n",
       "2  ../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...  hip-hop\n",
       "3  ../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...  hip-hop\n",
       "4  ../data/raw/1000_mel_spec_seg\\000RDCYioLteXcut...  hip-hop"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing images\n",
    "image_folder = \"../data/raw/1000_mel_spec_seg\"\n",
    "\n",
    "# List all image files in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "# Create a list to store rows for the new CSV file\n",
    "image_data = []\n",
    "\n",
    "# Loop through image files\n",
    "for image_file in image_files:\n",
    "    # Extract the track_id from the filename (everything before \"_segment\")\n",
    "    track_id = \"_\".join(image_file.split(\"_\")[:-2])  # Adjust splitting based on naming\n",
    "    \n",
    "    # Look up the corresponding genre or class in the dataframe\n",
    "    if track_id in dfg['track_id'].values:  # Assuming 'track_id' column exists in dfg\n",
    "        row = dfg[dfg['track_id'] == track_id].iloc[0]\n",
    "        \n",
    "        # Extract the class or genre information\n",
    "        class_info = row['track_genre']  # Replace with one-hot columns if needed\n",
    "        \n",
    "        # Add a row to the new data: [image path, class/genre]\n",
    "        image_data.append([os.path.join(image_folder, image_file), class_info])\n",
    "    else:\n",
    "        print(f\"Track ID {track_id} not found in the dataframe!\")  # Debugging info\n",
    "\n",
    "# Create a new dataframe for the association file\n",
    "association_df = pd.DataFrame(image_data, columns=[\"image_path\", \"class\"])\n",
    "association_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode association_df\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "association_df['class'] = le.fit_transform(association_df['class'])\n",
    "\n",
    "association_df.to_csv('../data/raw/Limage_class_association.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "dfg_e = pd.get_dummies(association_df, columns=['class'])\n",
    "\n",
    "# Identify the one-hot-encoded columns\n",
    "one_hot_columns = [col for col in dfg_e.columns if 'class' in col]\n",
    "\n",
    "# Convert only the one-hot-encoded columns to integers\n",
    "dfg_e[one_hot_columns] = dfg_e[one_hot_columns].astype(int)\n",
    "\n",
    "# Verify the result\n",
    "dfg_e.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association CSV file created: ../data/raw/Nimage_class_association.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the new dataframe to a CSV file\n",
    "association_csv_path = \"../data/raw/Nimage_class_association.csv\"\n",
    "dfg_e.to_csv(association_csv_path, index=False)\n",
    "\n",
    "print(f\"Association CSV file created: {association_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset + dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Step 1: Group images by track ID and class\n",
    "def group_tracks_by_class_and_id(association_csv):\n",
    "    df = pd.read_csv(association_csv)\n",
    "    df['track_id'] = df['image_path'].apply(lambda x: \"_\".join(os.path.basename(x).split(\"_\")[:-2]))\n",
    "    class_groups = defaultdict(list)\n",
    "    \n",
    "    # Group track IDs by their class\n",
    "    for track_id, group in df.groupby('track_id'):\n",
    "        track_class = group.iloc[0]['class']\n",
    "        class_groups[track_class].append(track_id)\n",
    "    \n",
    "    return class_groups, df\n",
    "\n",
    "# Step 2: Split track IDs for each class\n",
    "def split_tracks_by_class(class_groups, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):\n",
    "    train_ids, val_ids, test_ids = [], [], []\n",
    "    \n",
    "    for track_class, track_ids in class_groups.items():\n",
    "        random.shuffle(track_ids)  # Shuffle track IDs within the class\n",
    "        \n",
    "        # Perform splits\n",
    "        train, temp = train_test_split(track_ids, test_size=(1 - train_ratio))\n",
    "        val, test = train_test_split(temp, test_size=(test_ratio / (test_ratio + val_ratio)))\n",
    "        \n",
    "        # Append to respective splits\n",
    "        train_ids.extend(train)\n",
    "        val_ids.extend(val)\n",
    "        test_ids.extend(test)\n",
    "    \n",
    "    return train_ids, val_ids, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a custom PyTorch Dataset\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, df, track_ids, transform=None):\n",
    "        self.data = df[df['track_id'].isin(track_ids)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = row['image_path']\n",
    "        label = row['class']\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 6424\n",
      "Number of validation samples: 947\n",
      "Number of testing samples: 1875\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create DataLoaders\n",
    "def create_balanced_dataloaders(image_folder, association_csv, batch_size=32):\n",
    "    # Group by class and track ID\n",
    "    class_groups, df = group_tracks_by_class_and_id(association_csv)\n",
    "    \n",
    "    # Perform class-balanced splits\n",
    "    train_ids, val_ids, test_ids = split_tracks_by_class(class_groups)\n",
    "    \n",
    "    # Define image transformations (e.g., resizing, normalization)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to a consistent size\n",
    "        transforms.ToTensor(),         # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SpectrogramDataset(df, train_ids, transform=transform)\n",
    "    val_dataset = SpectrogramDataset(df, val_ids, transform=transform)\n",
    "    test_dataset = SpectrogramDataset(df, test_ids, transform=transform)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Example Usage\n",
    "image_folder = \"../data/raw/1000mel_spec_seg\"\n",
    "association_csv = \"../data/raw/Limage_class_association.csv\"\n",
    "\n",
    "train_loader, val_loader, test_loader = create_balanced_dataloaders(image_folder, association_csv)\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Number of training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-1.0000, -1.0000, -1.0000,  ..., -0.9373, -0.9216, -0.8353],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.9373, -0.9216, -0.8353],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.9216, -0.9216, -0.8431],\n",
      "         ...,\n",
      "         [-0.5686, -0.8745, -0.6863,  ...,  0.9294,  0.9529,  0.9608],\n",
      "         [-0.7098, -0.9686, -0.9373,  ...,  0.9137,  0.9373,  0.9451],\n",
      "         [-0.7490, -0.9922, -1.0000,  ...,  0.9137,  0.9294,  0.9451]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.9765, -0.9608],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.9765, -0.9608],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.9686, -0.9765, -0.9529],\n",
      "         ...,\n",
      "         [-0.8902, -0.9451, -0.9294,  ...,  0.6784,  0.5843,  0.5294],\n",
      "         [-0.9373, -0.9843, -0.9843,  ...,  0.7255,  0.6549,  0.5843],\n",
      "         [-0.9529, -0.9922, -1.0000,  ...,  0.7412,  0.6706,  0.6000]],\n",
      "\n",
      "        [[-0.9686, -0.9686, -0.9686,  ..., -0.8196, -0.8196, -0.6549],\n",
      "         [-0.9686, -0.9686, -0.9686,  ..., -0.8118, -0.8196, -0.6549],\n",
      "         [-0.9686, -0.9686, -0.9686,  ..., -0.7961, -0.8118, -0.6549],\n",
      "         ...,\n",
      "         [-0.4353, -0.6941, -0.3961,  ..., -0.4588, -0.6000, -0.6784],\n",
      "         [-0.6078, -0.8902, -0.8510,  ..., -0.3725, -0.4980, -0.6000],\n",
      "         [-0.6549, -0.9451, -0.9686,  ..., -0.3490, -0.4667, -0.5765]]]), 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = train_loader.dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check track_id excistence in the dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#from audioset_tagging_cnn.models import Cnn14  \n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final layer for the number of genres \n",
    "num_genres = 5  # Replace with the actual number of genres\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),  # Intermediate layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, 5)  # Output layer for genres\n",
    ")\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7d9vjmr3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▄▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▂▅██▇▁▄▃</td></tr><tr><td>val_loss</td><td>▆▄▄▄▁▁▁▅▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.61473</td></tr><tr><td>train_loss</td><td>0.89864</td></tr><tr><td>val_accuracy</td><td>0.41394</td></tr><tr><td>val_loss</td><td>1.90581</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-snowflake-4</strong> at: <a href='https://wandb.ai/babisbabis/Audio_Class/runs/7d9vjmr3' target=\"_blank\">https://wandb.ai/babisbabis/Audio_Class/runs/7d9vjmr3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241127_151228-7d9vjmr3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7d9vjmr3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b2bea0d89d492396e2b95378b1dd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01128888888860173, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\spbsp\\OneDrive - Danmarks Tekniske Universitet\\Skrivebord\\DiffAudio\\notebooks\\wandb\\run-20241127_154641-56alsbam</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/babisbabis/Audio_Class/runs/56alsbam' target=\"_blank\">icy-firefly-5</a></strong> to <a href='https://wandb.ai/babisbabis/Audio_Class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/babisbabis/Audio_Class' target=\"_blank\">https://wandb.ai/babisbabis/Audio_Class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/babisbabis/Audio_Class/runs/56alsbam' target=\"_blank\">https://wandb.ai/babisbabis/Audio_Class/runs/56alsbam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project='Audio_Class',  \n",
    "    \n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    # Add other hyperparameters as needed\n",
    "})\n",
    "\n",
    "# Access the configuration\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:56alsbam) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d6ff12949f4bab9b8d5ebdcd9d757b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▃▃▅▅▆▆██</td></tr><tr><td>train_accuracy</td><td>▁▄▆▇█</td></tr><tr><td>train_loss</td><td>█▅▃▂▁</td></tr><tr><td>val_accuracy</td><td>▁▄██▆</td></tr><tr><td>val_loss</td><td>▅▁▃▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>train_accuracy</td><td>0.842</td></tr><tr><td>train_loss</td><td>0.42554</td></tr><tr><td>val_accuracy</td><td>0.65576</td></tr><tr><td>val_loss</td><td>1.14189</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-firefly-5</strong> at: <a href='https://wandb.ai/babisbabis/Audio_Class/runs/56alsbam' target=\"_blank\">https://wandb.ai/babisbabis/Audio_Class/runs/56alsbam</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241127_154641-56alsbam\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:56alsbam). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909202e7a9e647c9908b10538725e74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\spbsp\\OneDrive - Danmarks Tekniske Universitet\\Skrivebord\\DiffAudio\\notebooks\\wandb\\run-20241127_155916-sany8mx4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/babisbabis/Audio_Class/runs/sany8mx4' target=\"_blank\">drawn-galaxy-6</a></strong> to <a href='https://wandb.ai/babisbabis/Audio_Class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/babisbabis/Audio_Class' target=\"_blank\">https://wandb.ai/babisbabis/Audio_Class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/babisbabis/Audio_Class/runs/sany8mx4' target=\"_blank\">https://wandb.ai/babisbabis/Audio_Class/runs/sany8mx4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project='Audio_Class',  \n",
    "    \n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    # Add other hyperparameters as needed\n",
    "})\n",
    "\n",
    "# Access the configuration\n",
    "config = wandb.config\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, save_interval=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        # Initialize tqdm progress bar for training\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\", leave=True)\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate metrics\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "            # Update tqdm bar\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "\n",
    "        # Initialize tqdm progress bar for validation\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", leave=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in progress_bar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Calculate metrics\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "                # Update tqdm bar\n",
    "                progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "        progress_bar.close()\n",
    "        \n",
    "                # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "        })\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Save the model at specific intervals\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_path = os.path.join(\"saved_models\", f\"model_epoch_{epoch+1}.pth\")\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_model(model, test_loader, device, class_names=None):\n",
    "    \"\"\"\n",
    "    Test the trained model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        device: Torch device (CPU or CUDA).\n",
    "        class_names: List of class names corresponding to the labels (optional).\n",
    "    \n",
    "    Returns:\n",
    "        None. Prints accuracy, precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Labels are already numeric (encoded 0-4), no need to transform\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)  # Get predicted class indices\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Optional: Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    if class_names:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "save_folder = \"../models/classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Assuming model, train_loader, val_loader, criterion, optimizer are defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████| 201/201 [01:38<00:00,  2.03it/s, Loss=0.1369]\n",
      "Epoch 1/10 - Validation: 100%|██████████| 30/30 [00:13<00:00,  2.17it/s, Loss=0.7075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: Train Loss: 0.1870, Train Accuracy: 0.9368, Val Loss: 1.0891, Val Accuracy: 0.7254\n",
      "Model saved to saved_models\\model_epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████| 201/201 [01:42<00:00,  1.97it/s, Loss=0.0836]\n",
      "Epoch 2/10 - Validation: 100%|██████████| 30/30 [00:13<00:00,  2.18it/s, Loss=0.7147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: Train Loss: 0.1076, Train Accuracy: 0.9626, Val Loss: 1.3618, Val Accuracy: 0.6864\n",
      "Model saved to saved_models\\model_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████| 201/201 [01:45<00:00,  1.90it/s, Loss=0.0478]\n",
      "Epoch 3/10 - Validation: 100%|██████████| 30/30 [00:13<00:00,  2.18it/s, Loss=0.6910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: Train Loss: 0.0743, Train Accuracy: 0.9757, Val Loss: 1.5726, Val Accuracy: 0.7107\n",
      "Model saved to saved_models\\model_epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████| 201/201 [01:37<00:00,  2.06it/s, Loss=0.0799]\n",
      "Epoch 4/10 - Validation: 100%|██████████| 30/30 [00:13<00:00,  2.18it/s, Loss=1.3393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: Train Loss: 0.0780, Train Accuracy: 0.9721, Val Loss: 1.1998, Val Accuracy: 0.7381\n",
      "Model saved to saved_models\\model_epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████| 201/201 [01:38<00:00,  2.04it/s, Loss=0.0244]\n",
      "Epoch 5/10 - Validation: 100%|██████████| 30/30 [00:13<00:00,  2.19it/s, Loss=0.8059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: Train Loss: 0.0678, Train Accuracy: 0.9785, Val Loss: 1.4489, Val Accuracy: 0.6938\n",
      "Model saved to saved_models\\model_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████| 201/201 [01:43<00:00,  1.94it/s, Loss=0.0406]\n",
      "Epoch 6/10 - Validation: 100%|██████████| 30/30 [00:15<00:00,  2.00it/s, Loss=1.6607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: Train Loss: 0.0359, Train Accuracy: 0.9899, Val Loss: 1.9341, Val Accuracy: 0.6948\n",
      "Model saved to saved_models\\model_epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████| 201/201 [01:42<00:00,  1.95it/s, Loss=0.0467]\n",
      "Epoch 7/10 - Validation: 100%|██████████| 30/30 [00:14<00:00,  2.07it/s, Loss=1.3770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: Train Loss: 0.0596, Train Accuracy: 0.9813, Val Loss: 1.3377, Val Accuracy: 0.7276\n",
      "Model saved to saved_models\\model_epoch_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████| 201/201 [01:49<00:00,  1.84it/s, Loss=0.0029]\n",
      "Epoch 8/10 - Validation: 100%|██████████| 30/30 [00:14<00:00,  2.07it/s, Loss=0.9072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: Train Loss: 0.0345, Train Accuracy: 0.9894, Val Loss: 1.5075, Val Accuracy: 0.7191\n",
      "Model saved to saved_models\\model_epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████| 201/201 [01:45<00:00,  1.90it/s, Loss=0.0976]\n",
      "Epoch 9/10 - Validation: 100%|██████████| 30/30 [00:14<00:00,  2.00it/s, Loss=1.8110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: Train Loss: 0.0296, Train Accuracy: 0.9896, Val Loss: 1.7748, Val Accuracy: 0.6917\n",
      "Model saved to saved_models\\model_epoch_9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|██████████| 201/201 [01:42<00:00,  1.96it/s, Loss=0.2599]\n",
      "Epoch 10/10 - Validation: 100%|██████████| 30/30 [00:14<00:00,  2.09it/s, Loss=1.0285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: Train Loss: 0.0581, Train Accuracy: 0.9818, Val Loss: 1.5320, Val Accuracy: 0.7043\n",
      "Model saved to saved_models\\model_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6287\n",
      "Test Precision: 0.6576\n",
      "Test Recall: 0.6287\n",
      "Test F1 Score: 0.6376\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
