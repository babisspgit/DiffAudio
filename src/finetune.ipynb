{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import wandb\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      " \n",
      "STDERR:\n",
      " C:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\python.exe: can't open file 'c:\\\\Users\\\\spbsp\\\\OneDrive - Danmarks Tekniske Universitet\\\\Skrivebord\\\\DiffAudio\\\\src\\\\model\\\\train_text_to_image.py': [Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\Scripts\\accelerate.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\accelerate\\commands\\accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"C:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 1168, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"C:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\accelerate\\commands\\launch.py\", line 763, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['C:\\\\Users\\\\spbsp\\\\anaconda3\\\\envs\\\\ddpm\\\\python.exe', 'train_text_to_image.py', '--pretrained_model_name_or_path=CompVis/stable-diffusion-v1-4', '--train_data_dir=../data/raw/finetunetrial.csv', '--use_ema', '--resolution=512 --center_crop --random_flip', '--train_batch_size=1', '--gradient_accumulation_steps=4', '--gradient_checkpointing', '--max_train_steps=15000', '--learning_rate=1e-05', '--max_grad_norm=1', '--lr_scheduler=constant --lr_warmup_steps=0', '--output_dir=results']' returned non-zero exit status 2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the command as a list of arguments\n",
    "command = [\n",
    "    \"accelerate\", \"launch\", \"train_text_to_image.py\",\n",
    "    \"--pretrained_model_name_or_path=CompVis/stable-diffusion-v1-4\",\n",
    "    \"--train_data_dir=../data/raw/finetunetrial.csv\",\n",
    "    \"--use_ema\",\n",
    "    \"--resolution=512 --center_crop --random_flip\",\n",
    "    \"--train_batch_size=1\",\n",
    "    \"--gradient_accumulation_steps=4\", \n",
    "    \"--gradient_checkpointing\",\n",
    "    \"--max_train_steps=15000\",\n",
    "    \"--learning_rate=1e-05\",\n",
    "    \"--max_grad_norm=1\",\n",
    "    \"--lr_scheduler=constant --lr_warmup_steps=0\", \n",
    "    \"--output_dir=results\"\n",
    "]\n",
    "\n",
    "# Run the command and capture stdout and stderr\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Print standard output and error\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accelerate launch train_dreambooth.py \\\n",
    "#    --pretrained_model_name_or_path=\"CompVis/stable-diffusion-v1-4\" \\\n",
    "#    --instance_data_dir=\"C:/Users\\spbsp/OneDrive - Danmarks Tekniske Universitet/Skrivebord/DiffAudio/data/raw/1000_mel\" \\\n",
    "#    --output_dir=\"./spectrogram_output\" \\\n",
    "#    --instance_prompt_file=\"C:/Users\\spbsp/OneDrive - Danmarks Tekniske Universitet/Skrivebord/DiffAudio/data/raw/1000dataset_prompts\" \\\n",
    "#    --train_text_encoder \\\n",
    "#    --resolution=512 \\\n",
    "#    --train_batch_size=1 \\\n",
    "#    --gradient_accumulation_steps=1 \\\n",
    "#    --learning_rate=5e-6 \\\n",
    "#    --max_train_steps=1000\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Define the command as a list of arguments\n",
    "command = [\n",
    "    \"accelerate\", \"launch\", \"train_dreambooth.py\",\n",
    "    \"--pretrained_model_name_or_path=CompVis/stable-diffusion-v1-4\",\n",
    "    \"--instance_data_dir=C:/Users/spbsp/OneDrive - Danmarks Tekniske Universitet/Skrivebord/DiffAudio/data/raw/1000_mel\",\n",
    "    \"--output_dir=./spectrogram_output\",\n",
    "    \"--train_text_encoder\",\n",
    "    \"--resolution=512\",\n",
    "    \"--train_batch_size=1\",\n",
    "    \"--gradient_accumulation_steps=1\",\n",
    "    \"--learning_rate=5e-6\",\n",
    "    \"--max_train_steps=1000\"\n",
    "]\n",
    "\n",
    "# Run the command and capture stdout and stderr\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Print standard output and error\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Model - Std Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load CSV with image paths and prompts\n",
    "data = pd.read_csv(\"../data/raw/finetunetrial.csv\")\n",
    "\n",
    "# Define a DatasetDict for train and validation (if you have separate sets)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data),\n",
    "    \"validation\": Dataset.from_pandas(data)  # or use a different CSV/file\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2eaec969cc84ac08d1b98764f887011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E59464AE60>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BED420>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BEC5E0>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BEC760>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BED420>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3C0C880>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e659f1de964caba56accf8355a7572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E5941C1BA0>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BED360>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BEE5C0>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BEE6B0>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BED360>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n",
      "<PIL.Image.Image image mode=RGB size=512x512 at 0x2E3A3BEF340>\n",
      "pixel_values type: <class 'torch.Tensor'>, input_ids type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer for text prompts\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def process_data(batch):\n",
    "    # Load and resize image\n",
    "    image = Image.open(f\"../data/raw/1000_mel_spec_seg/{batch['file_name']}\").convert(\"RGB\").resize((512, 512))\n",
    "    print(image)\n",
    "    \n",
    "    # Convert image to numpy array and then to tensor\n",
    "    image_np = np.array(image)  # Convert to NumPy array\n",
    "    batch[\"pixel_values\"] = torch.tensor(image_np).permute(2, 0, 1).float() / 255.0  # Normalize to [0,1]\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = tokenizer(batch[\"additional_feature\"], padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\")\n",
    "    batch[\"input_ids\"] = tokenized_prompt.input_ids[0]\n",
    "    \n",
    "    # Debugging: Check types\n",
    "    print(f\"pixel_values type: {type(batch['pixel_values'])}, input_ids type: {type(batch['input_ids'])}\")\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "    \n",
    "# Apply the processing function to the dataset\n",
    "dataset = dataset.map(process_data, batched=False)\n",
    "dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df71b3d5d65c40fbaccdbc7904b23816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67a5898f3064c8187c91fee0f46261c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  19%|#8        | 640M/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e1707e8b72455786a594208b29c032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  55%|#####5    | 273M/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d20434c19244d7ebc2018a3ea2f0419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  25%|##5       | 304M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\spbsp\\.cache\\huggingface\\hub\\models--CompVis--stable-diffusion-v1-4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e701243044234784ba86ce7f1dd35a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.32.0.dev0\",\n",
       "  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load the pretrained model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe.to(\"cuda\")  # Make sure you have a GPU for faster training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch types: [<class 'torch.Tensor'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spbsp\\AppData\\Local\\Temp\\ipykernel_51068\\209457800.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  noise_scale = torch.sqrt(torch.tensor(timestep / num_train_timesteps, device=\"cuda\"))\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2048.00 GiB (GPU 0; 6.00 GiB total capacity; 17.56 GiB already allocated; 0 bytes free; 17.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Apply noise to images (simulating a diffusion step)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m noisy_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([images, noise], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1216\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[1;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1214\u001b[0m         additional_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m down_intrablock_additional_residuals\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m-> 1216\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(\n\u001b[0;32m   1217\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m   1218\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[0;32m   1219\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1220\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1221\u001b[0m         cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcross_attention_kwargs,\n\u001b[0;32m   1222\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1223\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madditional_residuals,\n\u001b[0;32m   1224\u001b[0m     )\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1226\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb)\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:1288\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[1;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m-> 1288\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:442\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[0;32m    430\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    431\u001b[0m             create_custom_forward(block),\n\u001b[0;32m    432\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[0;32m    440\u001b[0m         )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\diffusers\\models\\attention.py:507\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    505\u001b[0m gligen_kwargs \u001b[38;5;241m=\u001b[39m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgligen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 507\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\n\u001b[0;32m    508\u001b[0m     norm_hidden_states,\n\u001b[0;32m    509\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    510\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    512\u001b[0m )\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    515\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m attn_output\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\diffusers\\models\\attention_processor.py:502\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    500\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    504\u001b[0m     hidden_states,\n\u001b[0;32m    505\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    506\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs,\n\u001b[0;32m    508\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\diffusers\\models\\attention_processor.py:779\u001b[0m, in \u001b[0;36mAttnProcessor.__call__\u001b[1;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[0;32m    776\u001b[0m key \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mhead_to_batch_dim(key)\n\u001b[0;32m    777\u001b[0m value \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mhead_to_batch_dim(value)\n\u001b[1;32m--> 779\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attention_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attention_probs, value)\n\u001b[0;32m    781\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mbatch_to_head_dim(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\diffusers\\models\\attention_processor.py:574\u001b[0m, in \u001b[0;36mAttention.get_attention_scores\u001b[1;34m(self, query, key, attention_mask)\u001b[0m\n\u001b[0;32m    571\u001b[0m     key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     baddbmm_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m     beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2048.00 GiB (GPU 0; 6.00 GiB total capacity; 17.56 GiB already allocated; 0 bytes free; 17.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    print(f\"Batch types: {[type(item['pixel_values']) for item in batch]}\")  # Check types\n",
    "    # Stack images and input IDs\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    }\n",
    "\n",
    "# Create DataLoader with collate_fn\n",
    "train_dataloader = DataLoader(dataset[\"train\"], batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(pipe.unet.parameters(), lr=5e-6)\n",
    "\n",
    "num_train_timesteps = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):  # Adjust based on your requirements\n",
    "    for batch in train_dataloader:\n",
    "        # Move images and tokens to the device\n",
    "        images = batch[\"pixel_values\"].to(\"cuda\")\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "        # Get text embeddings\n",
    "        text_embeddings = pipe.text_encoder(input_ids)[0]\n",
    "\n",
    "        # Generate 1-channel noise and sample a random timestep\n",
    "         # Scale noise based on timestep\n",
    "        noise_scale = torch.sqrt(torch.tensor(timestep / num_train_timesteps, device=\"cuda\"))\n",
    "        noise = torch.randn((images.shape[0], 1, images.shape[2], images.shape[3]), device=\"cuda\") * noise_scale  # Shape: (batch_size, 1, 512, 512)\n",
    "        timestep = torch.randint(0, num_train_timesteps, (1,), device=\"cuda\").long()  # Random timestep\n",
    "\n",
    "        # Apply noise to images (simulating a diffusion step)\n",
    "        noisy_images = torch.cat([images, noise], dim=1)\n",
    "        loss = pipe.unet(noisy_images, timestep=timestep, encoder_hidden_states=text_embeddings).loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed with loss {loss.item()}\")\n",
    "    \n",
    "# Save model\n",
    "pipe.save_pretrained(\"fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Code\n",
    "\n",
    "# Generate images using the fine-tuned model\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image-to-image pipeline and apply LoRA weights\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.unet.from_pretrained(\"lora_fine_tuned_model\")\n",
    "\n",
    "# Load the initial image\n",
    "initial_image = Image.open(\"path/to/your/initial_image.jpg\").convert(\"RGB\")\n",
    "initial_image = initial_image.resize((512, 512))  # Resize to model's expected input size\n",
    "\n",
    "# Define the text prompt\n",
    "prompt = \"A calm melody with soft piano notes in a beautiful spectrogram style\"\n",
    "\n",
    "# Run the image-to-image generation with the prompt and initial image\n",
    "output = pipe(prompt=prompt, init_image=initial_image, strength=0.75, guidance_scale=7.5)\n",
    "\n",
    "# Save the generated image\n",
    "generated_image = output.images[0]\n",
    "generated_image.save(\"generated_spectrogram.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Model - Finetuning Using LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate peft pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import CLIPTokenizer\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1bedc298b54c308b32c81e708c50b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv_in\n",
      "time_proj\n",
      "time_embedding\n",
      "time_embedding.linear_1\n",
      "time_embedding.act\n",
      "time_embedding.linear_2\n",
      "down_blocks\n",
      "down_blocks.0\n",
      "down_blocks.0.attentions\n",
      "down_blocks.0.attentions.0\n",
      "down_blocks.0.attentions.0.norm\n",
      "down_blocks.0.attentions.0.proj_in\n",
      "down_blocks.0.attentions.0.transformer_blocks\n",
      "down_blocks.0.attentions.0.transformer_blocks.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.1\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2\n",
      "down_blocks.0.attentions.0.proj_out\n",
      "down_blocks.0.attentions.1\n",
      "down_blocks.0.attentions.1.norm\n",
      "down_blocks.0.attentions.1.proj_in\n",
      "down_blocks.0.attentions.1.transformer_blocks\n",
      "down_blocks.0.attentions.1.transformer_blocks.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.1\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2\n",
      "down_blocks.0.attentions.1.proj_out\n",
      "down_blocks.0.resnets\n",
      "down_blocks.0.resnets.0\n",
      "down_blocks.0.resnets.0.norm1\n",
      "down_blocks.0.resnets.0.conv1\n",
      "down_blocks.0.resnets.0.time_emb_proj\n",
      "down_blocks.0.resnets.0.norm2\n",
      "down_blocks.0.resnets.0.dropout\n",
      "down_blocks.0.resnets.0.conv2\n",
      "down_blocks.0.resnets.1\n",
      "down_blocks.0.resnets.1.norm1\n",
      "down_blocks.0.resnets.1.conv1\n",
      "down_blocks.0.resnets.1.time_emb_proj\n",
      "down_blocks.0.resnets.1.norm2\n",
      "down_blocks.0.resnets.1.dropout\n",
      "down_blocks.0.resnets.1.conv2\n",
      "down_blocks.0.downsamplers\n",
      "down_blocks.0.downsamplers.0\n",
      "down_blocks.0.downsamplers.0.conv\n",
      "down_blocks.1\n",
      "down_blocks.1.attentions\n",
      "down_blocks.1.attentions.0\n",
      "down_blocks.1.attentions.0.norm\n",
      "down_blocks.1.attentions.0.proj_in\n",
      "down_blocks.1.attentions.0.transformer_blocks\n",
      "down_blocks.1.attentions.0.transformer_blocks.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.1\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2\n",
      "down_blocks.1.attentions.0.proj_out\n",
      "down_blocks.1.attentions.1\n",
      "down_blocks.1.attentions.1.norm\n",
      "down_blocks.1.attentions.1.proj_in\n",
      "down_blocks.1.attentions.1.transformer_blocks\n",
      "down_blocks.1.attentions.1.transformer_blocks.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.1\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2\n",
      "down_blocks.1.attentions.1.proj_out\n",
      "down_blocks.1.resnets\n",
      "down_blocks.1.resnets.0\n",
      "down_blocks.1.resnets.0.norm1\n",
      "down_blocks.1.resnets.0.conv1\n",
      "down_blocks.1.resnets.0.time_emb_proj\n",
      "down_blocks.1.resnets.0.norm2\n",
      "down_blocks.1.resnets.0.dropout\n",
      "down_blocks.1.resnets.0.conv2\n",
      "down_blocks.1.resnets.0.conv_shortcut\n",
      "down_blocks.1.resnets.1\n",
      "down_blocks.1.resnets.1.norm1\n",
      "down_blocks.1.resnets.1.conv1\n",
      "down_blocks.1.resnets.1.time_emb_proj\n",
      "down_blocks.1.resnets.1.norm2\n",
      "down_blocks.1.resnets.1.dropout\n",
      "down_blocks.1.resnets.1.conv2\n",
      "down_blocks.1.downsamplers\n",
      "down_blocks.1.downsamplers.0\n",
      "down_blocks.1.downsamplers.0.conv\n",
      "down_blocks.2\n",
      "down_blocks.2.attentions\n",
      "down_blocks.2.attentions.0\n",
      "down_blocks.2.attentions.0.norm\n",
      "down_blocks.2.attentions.0.proj_in\n",
      "down_blocks.2.attentions.0.transformer_blocks\n",
      "down_blocks.2.attentions.0.transformer_blocks.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.1\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2\n",
      "down_blocks.2.attentions.0.proj_out\n",
      "down_blocks.2.attentions.1\n",
      "down_blocks.2.attentions.1.norm\n",
      "down_blocks.2.attentions.1.proj_in\n",
      "down_blocks.2.attentions.1.transformer_blocks\n",
      "down_blocks.2.attentions.1.transformer_blocks.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.1\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2\n",
      "down_blocks.2.attentions.1.proj_out\n",
      "down_blocks.2.resnets\n",
      "down_blocks.2.resnets.0\n",
      "down_blocks.2.resnets.0.norm1\n",
      "down_blocks.2.resnets.0.conv1\n",
      "down_blocks.2.resnets.0.time_emb_proj\n",
      "down_blocks.2.resnets.0.norm2\n",
      "down_blocks.2.resnets.0.dropout\n",
      "down_blocks.2.resnets.0.conv2\n",
      "down_blocks.2.resnets.0.conv_shortcut\n",
      "down_blocks.2.resnets.1\n",
      "down_blocks.2.resnets.1.norm1\n",
      "down_blocks.2.resnets.1.conv1\n",
      "down_blocks.2.resnets.1.time_emb_proj\n",
      "down_blocks.2.resnets.1.norm2\n",
      "down_blocks.2.resnets.1.dropout\n",
      "down_blocks.2.resnets.1.conv2\n",
      "down_blocks.2.downsamplers\n",
      "down_blocks.2.downsamplers.0\n",
      "down_blocks.2.downsamplers.0.conv\n",
      "down_blocks.3\n",
      "down_blocks.3.resnets\n",
      "down_blocks.3.resnets.0\n",
      "down_blocks.3.resnets.0.norm1\n",
      "down_blocks.3.resnets.0.conv1\n",
      "down_blocks.3.resnets.0.time_emb_proj\n",
      "down_blocks.3.resnets.0.norm2\n",
      "down_blocks.3.resnets.0.dropout\n",
      "down_blocks.3.resnets.0.conv2\n",
      "down_blocks.3.resnets.1\n",
      "down_blocks.3.resnets.1.norm1\n",
      "down_blocks.3.resnets.1.conv1\n",
      "down_blocks.3.resnets.1.time_emb_proj\n",
      "down_blocks.3.resnets.1.norm2\n",
      "down_blocks.3.resnets.1.dropout\n",
      "down_blocks.3.resnets.1.conv2\n",
      "up_blocks\n",
      "up_blocks.0\n",
      "up_blocks.0.resnets\n",
      "up_blocks.0.resnets.0\n",
      "up_blocks.0.resnets.0.norm1\n",
      "up_blocks.0.resnets.0.conv1\n",
      "up_blocks.0.resnets.0.time_emb_proj\n",
      "up_blocks.0.resnets.0.norm2\n",
      "up_blocks.0.resnets.0.dropout\n",
      "up_blocks.0.resnets.0.conv2\n",
      "up_blocks.0.resnets.0.conv_shortcut\n",
      "up_blocks.0.resnets.1\n",
      "up_blocks.0.resnets.1.norm1\n",
      "up_blocks.0.resnets.1.conv1\n",
      "up_blocks.0.resnets.1.time_emb_proj\n",
      "up_blocks.0.resnets.1.norm2\n",
      "up_blocks.0.resnets.1.dropout\n",
      "up_blocks.0.resnets.1.conv2\n",
      "up_blocks.0.resnets.1.conv_shortcut\n",
      "up_blocks.0.resnets.2\n",
      "up_blocks.0.resnets.2.norm1\n",
      "up_blocks.0.resnets.2.conv1\n",
      "up_blocks.0.resnets.2.time_emb_proj\n",
      "up_blocks.0.resnets.2.norm2\n",
      "up_blocks.0.resnets.2.dropout\n",
      "up_blocks.0.resnets.2.conv2\n",
      "up_blocks.0.resnets.2.conv_shortcut\n",
      "up_blocks.0.upsamplers\n",
      "up_blocks.0.upsamplers.0\n",
      "up_blocks.0.upsamplers.0.conv\n",
      "up_blocks.1\n",
      "up_blocks.1.attentions\n",
      "up_blocks.1.attentions.0\n",
      "up_blocks.1.attentions.0.norm\n",
      "up_blocks.1.attentions.0.proj_in\n",
      "up_blocks.1.attentions.0.transformer_blocks\n",
      "up_blocks.1.attentions.0.transformer_blocks.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.1\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2\n",
      "up_blocks.1.attentions.0.proj_out\n",
      "up_blocks.1.attentions.1\n",
      "up_blocks.1.attentions.1.norm\n",
      "up_blocks.1.attentions.1.proj_in\n",
      "up_blocks.1.attentions.1.transformer_blocks\n",
      "up_blocks.1.attentions.1.transformer_blocks.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.1\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2\n",
      "up_blocks.1.attentions.1.proj_out\n",
      "up_blocks.1.attentions.2\n",
      "up_blocks.1.attentions.2.norm\n",
      "up_blocks.1.attentions.2.proj_in\n",
      "up_blocks.1.attentions.2.transformer_blocks\n",
      "up_blocks.1.attentions.2.transformer_blocks.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.1\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2\n",
      "up_blocks.1.attentions.2.proj_out\n",
      "up_blocks.1.resnets\n",
      "up_blocks.1.resnets.0\n",
      "up_blocks.1.resnets.0.norm1\n",
      "up_blocks.1.resnets.0.conv1\n",
      "up_blocks.1.resnets.0.time_emb_proj\n",
      "up_blocks.1.resnets.0.norm2\n",
      "up_blocks.1.resnets.0.dropout\n",
      "up_blocks.1.resnets.0.conv2\n",
      "up_blocks.1.resnets.0.conv_shortcut\n",
      "up_blocks.1.resnets.1\n",
      "up_blocks.1.resnets.1.norm1\n",
      "up_blocks.1.resnets.1.conv1\n",
      "up_blocks.1.resnets.1.time_emb_proj\n",
      "up_blocks.1.resnets.1.norm2\n",
      "up_blocks.1.resnets.1.dropout\n",
      "up_blocks.1.resnets.1.conv2\n",
      "up_blocks.1.resnets.1.conv_shortcut\n",
      "up_blocks.1.resnets.2\n",
      "up_blocks.1.resnets.2.norm1\n",
      "up_blocks.1.resnets.2.conv1\n",
      "up_blocks.1.resnets.2.time_emb_proj\n",
      "up_blocks.1.resnets.2.norm2\n",
      "up_blocks.1.resnets.2.dropout\n",
      "up_blocks.1.resnets.2.conv2\n",
      "up_blocks.1.resnets.2.conv_shortcut\n",
      "up_blocks.1.upsamplers\n",
      "up_blocks.1.upsamplers.0\n",
      "up_blocks.1.upsamplers.0.conv\n",
      "up_blocks.2\n",
      "up_blocks.2.attentions\n",
      "up_blocks.2.attentions.0\n",
      "up_blocks.2.attentions.0.norm\n",
      "up_blocks.2.attentions.0.proj_in\n",
      "up_blocks.2.attentions.0.transformer_blocks\n",
      "up_blocks.2.attentions.0.transformer_blocks.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.1\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2\n",
      "up_blocks.2.attentions.0.proj_out\n",
      "up_blocks.2.attentions.1\n",
      "up_blocks.2.attentions.1.norm\n",
      "up_blocks.2.attentions.1.proj_in\n",
      "up_blocks.2.attentions.1.transformer_blocks\n",
      "up_blocks.2.attentions.1.transformer_blocks.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.1\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2\n",
      "up_blocks.2.attentions.1.proj_out\n",
      "up_blocks.2.attentions.2\n",
      "up_blocks.2.attentions.2.norm\n",
      "up_blocks.2.attentions.2.proj_in\n",
      "up_blocks.2.attentions.2.transformer_blocks\n",
      "up_blocks.2.attentions.2.transformer_blocks.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.1\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2\n",
      "up_blocks.2.attentions.2.proj_out\n",
      "up_blocks.2.resnets\n",
      "up_blocks.2.resnets.0\n",
      "up_blocks.2.resnets.0.norm1\n",
      "up_blocks.2.resnets.0.conv1\n",
      "up_blocks.2.resnets.0.time_emb_proj\n",
      "up_blocks.2.resnets.0.norm2\n",
      "up_blocks.2.resnets.0.dropout\n",
      "up_blocks.2.resnets.0.conv2\n",
      "up_blocks.2.resnets.0.conv_shortcut\n",
      "up_blocks.2.resnets.1\n",
      "up_blocks.2.resnets.1.norm1\n",
      "up_blocks.2.resnets.1.conv1\n",
      "up_blocks.2.resnets.1.time_emb_proj\n",
      "up_blocks.2.resnets.1.norm2\n",
      "up_blocks.2.resnets.1.dropout\n",
      "up_blocks.2.resnets.1.conv2\n",
      "up_blocks.2.resnets.1.conv_shortcut\n",
      "up_blocks.2.resnets.2\n",
      "up_blocks.2.resnets.2.norm1\n",
      "up_blocks.2.resnets.2.conv1\n",
      "up_blocks.2.resnets.2.time_emb_proj\n",
      "up_blocks.2.resnets.2.norm2\n",
      "up_blocks.2.resnets.2.dropout\n",
      "up_blocks.2.resnets.2.conv2\n",
      "up_blocks.2.resnets.2.conv_shortcut\n",
      "up_blocks.2.upsamplers\n",
      "up_blocks.2.upsamplers.0\n",
      "up_blocks.2.upsamplers.0.conv\n",
      "up_blocks.3\n",
      "up_blocks.3.attentions\n",
      "up_blocks.3.attentions.0\n",
      "up_blocks.3.attentions.0.norm\n",
      "up_blocks.3.attentions.0.proj_in\n",
      "up_blocks.3.attentions.0.transformer_blocks\n",
      "up_blocks.3.attentions.0.transformer_blocks.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.1\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2\n",
      "up_blocks.3.attentions.0.proj_out\n",
      "up_blocks.3.attentions.1\n",
      "up_blocks.3.attentions.1.norm\n",
      "up_blocks.3.attentions.1.proj_in\n",
      "up_blocks.3.attentions.1.transformer_blocks\n",
      "up_blocks.3.attentions.1.transformer_blocks.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.1\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2\n",
      "up_blocks.3.attentions.1.proj_out\n",
      "up_blocks.3.attentions.2\n",
      "up_blocks.3.attentions.2.norm\n",
      "up_blocks.3.attentions.2.proj_in\n",
      "up_blocks.3.attentions.2.transformer_blocks\n",
      "up_blocks.3.attentions.2.transformer_blocks.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.1\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2\n",
      "up_blocks.3.attentions.2.proj_out\n",
      "up_blocks.3.resnets\n",
      "up_blocks.3.resnets.0\n",
      "up_blocks.3.resnets.0.norm1\n",
      "up_blocks.3.resnets.0.conv1\n",
      "up_blocks.3.resnets.0.time_emb_proj\n",
      "up_blocks.3.resnets.0.norm2\n",
      "up_blocks.3.resnets.0.dropout\n",
      "up_blocks.3.resnets.0.conv2\n",
      "up_blocks.3.resnets.0.conv_shortcut\n",
      "up_blocks.3.resnets.1\n",
      "up_blocks.3.resnets.1.norm1\n",
      "up_blocks.3.resnets.1.conv1\n",
      "up_blocks.3.resnets.1.time_emb_proj\n",
      "up_blocks.3.resnets.1.norm2\n",
      "up_blocks.3.resnets.1.dropout\n",
      "up_blocks.3.resnets.1.conv2\n",
      "up_blocks.3.resnets.1.conv_shortcut\n",
      "up_blocks.3.resnets.2\n",
      "up_blocks.3.resnets.2.norm1\n",
      "up_blocks.3.resnets.2.conv1\n",
      "up_blocks.3.resnets.2.time_emb_proj\n",
      "up_blocks.3.resnets.2.norm2\n",
      "up_blocks.3.resnets.2.dropout\n",
      "up_blocks.3.resnets.2.conv2\n",
      "up_blocks.3.resnets.2.conv_shortcut\n",
      "mid_block\n",
      "mid_block.attentions\n",
      "mid_block.attentions.0\n",
      "mid_block.attentions.0.norm\n",
      "mid_block.attentions.0.proj_in\n",
      "mid_block.attentions.0.transformer_blocks\n",
      "mid_block.attentions.0.transformer_blocks.0\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.1\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.1\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3\n",
      "mid_block.attentions.0.transformer_blocks.0.ff\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.1\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2\n",
      "mid_block.attentions.0.proj_out\n",
      "mid_block.resnets\n",
      "mid_block.resnets.0\n",
      "mid_block.resnets.0.norm1\n",
      "mid_block.resnets.0.conv1\n",
      "mid_block.resnets.0.time_emb_proj\n",
      "mid_block.resnets.0.norm2\n",
      "mid_block.resnets.0.dropout\n",
      "mid_block.resnets.0.conv2\n",
      "mid_block.resnets.1\n",
      "mid_block.resnets.1.norm1\n",
      "mid_block.resnets.1.conv1\n",
      "mid_block.resnets.1.time_emb_proj\n",
      "mid_block.resnets.1.norm2\n",
      "mid_block.resnets.1.dropout\n",
      "mid_block.resnets.1.conv2\n",
      "conv_norm_out\n",
      "conv_out\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules {'cross_attention'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 23\u001b[0m\n\u001b[0;32m     15\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m     16\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,                    \u001b[38;5;66;03m# Low-rank parameter\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,          \u001b[38;5;66;03m# LoRA scaling factor\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# Modules to apply LoRA on\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m        \u001b[38;5;66;03m# Dropout for regularization\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Apply LoRA to the pipeline\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m pipe\u001b[38;5;241m.\u001b[39munet \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\peft\\mapping.py:129\u001b[0m, in \u001b[0;36mget_peft_model\u001b[1;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftMixedModel(model, peft_config, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m    132\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\peft\\peft_model.py:123\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\peft\\tuners\\lora\\model.py:119\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[1;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:95\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[1;32mc:\\Users\\spbsp\\anaconda3\\envs\\ddpm\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:255\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[1;34m(self, model, adapter_name)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptional_kwargs)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m     )\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_only_adapters_as_trainable()\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39minference_mode:\n",
      "\u001b[1;31mValueError\u001b[0m: Target modules {'cross_attention'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "# Set Up the LoRA Fine-Tuning Model\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load the base model\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "for name, module in pipe.unet.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Set up LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                    # Low-rank parameter\n",
    "    lora_alpha=16,          # LoRA scaling factor\n",
    "    target_modules=[\"cross_attention\"],  # Modules to apply LoRA on\n",
    "    lora_dropout=0.1        # Dropout for regularization\n",
    ")\n",
    "\n",
    "# Apply LoRA to the pipeline\n",
    "pipe.unet = get_peft_model(pipe.unet, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning Loop with LoRA\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Optimizer (only update LoRA parameters)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, pipe.unet.parameters()), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    for batch in train_dataloader:\n",
    "        images = batch[\"pixel_values\"].to(\"cuda\")  # Move images to GPU\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")  # Move input IDs to GPU\n",
    "\n",
    "        # Generate text embeddings\n",
    "        text_embeddings = pipe.text_encoder(input_ids)[0]\n",
    "\n",
    "        # Generate noise and apply it to images\n",
    "        noise = torch.randn(images.shape).to(\"cuda\")\n",
    "        noisy_images = images + noise\n",
    "        loss = pipe.unet(noisy_images, text_embeddings).loss  # Calculate loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed with loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.save_pretrained(\"lora_fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Code\n",
    "\n",
    "# Generate images using the fine-tuned model\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image-to-image pipeline and apply LoRA weights\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.unet.from_pretrained(\"lora_fine_tuned_model\")\n",
    "\n",
    "# Load the initial image\n",
    "initial_image = Image.open(\"path/to/your/initial_image.jpg\").convert(\"RGB\")\n",
    "initial_image = initial_image.resize((512, 512))  # Resize to model's expected input size\n",
    "\n",
    "# Define the text prompt\n",
    "prompt = \"A calm melody with soft piano notes in a beautiful spectrogram style\"\n",
    "\n",
    "# Run the image-to-image generation with the prompt and initial image\n",
    "output = pipe(prompt=prompt, init_image=initial_image, strength=0.75, guidance_scale=7.5)\n",
    "\n",
    "# Save the generated image\n",
    "generated_image = output.images[0]\n",
    "generated_image.save(\"generated_spectrogram.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
